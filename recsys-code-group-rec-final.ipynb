{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom networkx.algorithms import bipartite\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport random","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:09:15.281913Z","iopub.execute_input":"2023-10-15T17:09:15.282420Z","iopub.status.idle":"2023-10-15T17:09:21.725976Z","shell.execute_reply.started":"2023-10-15T17:09:15.282375Z","shell.execute_reply":"2023-10-15T17:09:21.724454Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/movielens-20m-dataset/rating.csv')\ndata = data.iloc[:10000]\nmovies_df = pd.read_csv(\"/kaggle/input/movielens-20m-dataset/movie.csv\")\ntags_df = pd.read_csv(\"/kaggle/input/movielens-20m-dataset/tag.csv\")\ngenome_scores_raw = pd.read_csv(\"/kaggle/input/movielens-20m-dataset/genome_scores.csv\")\n# Filter genome_scores to only have movieIds present in the data dataframe\ngenome_scores = genome_scores_raw[genome_scores_raw['movieId'].isin(data['movieId'].unique())]\n\ntrain_data = data.copy()\ntest_data = pd.DataFrame()\n \nmovie_num_user_rated_counts = train_data['movieId'].value_counts()\n# Create a list of movie IDs with 10 or more ratings\npopular_movies = movie_num_user_rated_counts[movie_num_user_rated_counts >= 10].index.tolist()\n# Filter the DataFrame to keep only the rows with movies that meet the threshold\ntrain_data = train_data[train_data['movieId'].isin(popular_movies)]\n\nfor user_id in train_data['userId'].unique():\n    user_ratings = train_data[train_data['userId'] == user_id]\n    \n    if len(user_ratings) > 1:\n        test_rating = user_ratings.sample()\n        test_data = pd.concat([test_data, test_rating])\n        train_data.drop(test_rating.index, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:09:21.728621Z","iopub.execute_input":"2023-10-15T17:09:21.730343Z","iopub.status.idle":"2023-10-15T17:09:54.792055Z","shell.execute_reply.started":"2023-10-15T17:09:21.730289Z","shell.execute_reply":"2023-10-15T17:09:54.790203Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n\n# # Load the data\n# data = pd.read_csv('/kaggle/input/movielens-20m-dataset/rating.csv')\n# movies_df = pd.read_csv(\"/kaggle/input/movielens-20m-dataset/movie.csv\")\n# tags_df = pd.read_csv(\"/kaggle/input/movielens-20m-dataset/tag.csv\")\n\n# # Convert the timestamp column to datetime format for sorting\n# data['timestamp'] = pd.to_datetime(data['timestamp'])\n\n# # Filter users who have watched at least 30 movies\n# user_counts = data['userId'].value_counts()\n# eligible_users = user_counts[user_counts >= 30].index.tolist()\n\n# # Initialize train and test dataframes\n# train_data = pd.DataFrame()\n# test_data = pd.DataFrame()\n\n# # Iterate over eligible users and split their data\n# for user_id in eligible_users:\n#     user_data = data[data['userId'] == user_id].sort_values(by='timestamp', ascending=False)\n    \n#     # Add the latest 3 movies to the test set\n#     test_data = pd.concat([test_data, user_data.iloc[:3]])\n    \n#     # Add the remaining movies to the train set\n#     train_data = pd.concat([train_data, user_data.iloc[3:]])\n    \n#     # Check if train_data has reached the desired size\n#     if 10000 <= len(train_data) <= 50000:\n#         break\n\n# # Filter the train_data to keep only popular movies\n# movie_num_user_rated_counts = train_data['movieId'].value_counts()\n# popular_movies = movie_num_user_rated_counts[movie_num_user_rated_counts >= 5].index.tolist()\n# train_data = train_data[train_data['movieId'].isin(popular_movies)]\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-15T17:09:54.794346Z","iopub.execute_input":"2023-10-15T17:09:54.794994Z","iopub.status.idle":"2023-10-15T17:09:54.802062Z","shell.execute_reply.started":"2023-10-15T17:09:54.794923Z","shell.execute_reply":"2023-10-15T17:09:54.800843Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(train_data)","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:09:54.805047Z","iopub.execute_input":"2023-10-15T17:09:54.805634Z","iopub.status.idle":"2023-10-15T17:09:54.833931Z","shell.execute_reply.started":"2023-10-15T17:09:54.805600Z","shell.execute_reply":"2023-10-15T17:09:54.832363Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"      userId  movieId  rating            timestamp\n2          1       32     3.5  2005-04-02 23:33:39\n3          1       47     3.5  2005-04-02 23:32:07\n6          1      151     4.0  2004-09-10 03:08:54\n7          1      223     4.0  2005-04-02 23:46:13\n8          1      253     4.0  2005-04-02 23:35:40\n...      ...      ...     ...                  ...\n9989      91     2716     3.5  2005-03-28 23:17:18\n9990      91     2762     4.0  2005-03-22 22:48:33\n9994      91     2791     3.0  2005-07-18 08:13:22\n9995      91     2797     3.5  2005-03-29 01:53:41\n9997      91     2858     4.5  2005-03-23 05:57:57\n\n[3557 rows x 4 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"ratings_df = train_data\n# Create a new graph\nG = nx.Graph()\n\n# 2. Adding nodes and edges\n# Add user nodes\nfor user_id in ratings_df['userId'].unique():\n    G.add_node('u_'+str(user_id), bipartite=0)  # Add user node with a bipartite attribute of 0\n\n# Add movie nodes with title and genres as attributes\nfor _, row in movies_df.iterrows():\n    movie_id = 'm_' + str(row['movieId'])\n    G.add_node(movie_id, bipartite=1, title=row['title'], genres=row['genres'].split('|'))\n\n# Add edges based on ratings with rating and timestamp as attributes\nfor _, row in ratings_df.iterrows():\n    user_id = 'u_'+str(row['userId'])\n    movie_id = 'm_' + str(row['movieId'])\n    G.add_edge(user_id, movie_id, rating=row['rating'], timestamp=row['timestamp'])\n\n# Add tag data as an attribute to the movie nodes\nfor _, row in tags_df.iterrows():\n    movie_id = 'm_' + str(row['movieId'])\n    if 'tags' not in G.nodes[movie_id]:\n        G.nodes[movie_id]['tags'] = []\n    G.nodes[movie_id]['tags'].append({'tag': row['tag'], 'timestamp': row['timestamp'], 'userId': row['userId']})\n\n# Ensure the graph is bipartite\nassert bipartite.is_bipartite(G)","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:09:54.836080Z","iopub.execute_input":"2023-10-15T17:09:54.836570Z","iopub.status.idle":"2023-10-15T17:10:28.370364Z","shell.execute_reply.started":"2023-10-15T17:09:54.836535Z","shell.execute_reply":"2023-10-15T17:10:28.368864Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def predict_rating(G, user, movie):\n    neighbors = list(G.neighbors(movie))\n    if not neighbors:\n        return np.mean([attr['rating'] for _, _, attr in G.edges(data=True) if 'rating' in attr])\n\n    sim_weights = []\n    user_ratings = []\n    for neighbor in neighbors:\n        # Jaccard similarity as an example, but can be changed\n        common_movies = list(nx.common_neighbors(G, user, neighbor))\n        sim = len(common_movies) / (G.degree(user) + G.degree(neighbor) - len(common_movies))\n        rating = G[neighbor][movie]['rating']\n        \n        sim_weights.append(sim)\n        user_ratings.append(rating)\n    \n    return np.dot(user_ratings, sim_weights) / sum(sim_weights)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:10:28.372202Z","iopub.execute_input":"2023-10-15T17:10:28.372589Z","iopub.status.idle":"2023-10-15T17:10:28.381486Z","shell.execute_reply.started":"2023-10-15T17:10:28.372557Z","shell.execute_reply":"2023-10-15T17:10:28.380050Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def random_walk_legacy(G, start_node, alpha=0.85, walk_length=10):\n    \"\"\"Perform a random walk on graph G starting from node start_node.\"\"\"\n    current_node = start_node\n    path = [current_node]\n    \n    for _ in range(walk_length):\n        neighbors = list(G.neighbors(current_node))\n        if random.random() < alpha and neighbors:\n            current_node = random.choice(neighbors)\n        else:\n            current_node = start_node\n        path.append(current_node)\n    \n    return path\ndef weighted_choice(neighbors, weights):\n    \"\"\"Choose a neighbor based on the given weights.\"\"\"\n    total = sum(weights)\n    r = random.uniform(0, total)\n    upto = 0\n    for n, w in zip(neighbors, weights):\n        if upto + w >= r:\n            return n\n        upto += w\n\ndef random_walk(G, start_node, alpha=0.85, walk_length=10):\n    \"\"\"Perform a random walk on graph G starting from node start_node.\"\"\"\n    current_node = start_node\n    path = [current_node]\n    \n    for _ in range(walk_length):\n        neighbors = list(G.neighbors(current_node))\n        \n        if neighbors:\n            # Get weights (ratings) of the edges to the neighbors\n            weights = [G[current_node][neighbor].get('rating', 1) for neighbor in neighbors]\n            \n            if random.random() < alpha:\n                current_node = weighted_choice(neighbors, weights)\n            else:\n                current_node = start_node\n        else:\n            current_node = start_node\n            \n        path.append(current_node)\n    \n    return path\ndef personalized_pagerank_recommendations(G, user, alpha=0.85, num_walks=50, walk_length=5):\n    \"\"\"Generate recommendations and explanations using Personalized PageRank via random walks.\"\"\"\n    # Perform random walks and keep track of visit counts\n    visit_counts = {node: 0 for node in G.nodes()}\n    all_paths = []\n    user = 'u_'+str(user)\n    \n    for _ in range(num_walks):\n        path = random_walk(G, user, alpha, walk_length)\n        all_paths.append(path)\n        for node in path:\n            visit_counts[node] += 1\n    \n    # Normalize visit counts to get a probability distribution\n    total_visits = sum(visit_counts.values())\n    ppr = {node: count/total_visits for node, count in visit_counts.items()}\n    \n    # Filter for movies and sort by PPR score\n    movies = [node for node in ppr.keys() if G.nodes[node]['bipartite'] == 1 and node not in G.neighbors(str(user))]\n    sorted_movies = sorted(movies, key=lambda x: ppr[x], reverse=True)\n    \n    # Generate explanations for the top 10 movies\n    explanations = {}\n    significant_neighbors = {}\n    contributing_paths_all = {}\n    for movie in sorted_movies[:10]:\n        # Find paths that contributed to the movie's score\n        contributing_paths = [path for path in all_paths if movie in path]\n        \n        # Count the frequency of each neighbor leading to the movie\n        neighbor_counts = {}\n        for path in contributing_paths:\n            for i in range(len(path) - 1):\n                if path[i+1] == movie:\n                    neighbor = path[1]\n                    neighbor_counts[neighbor] = neighbor_counts.get(neighbor, 0) + 1\n\n        # Identify the most significant neighbor\n        sorted_neighbors = sorted(neighbor_counts, key=neighbor_counts.get, reverse=True)\n        most_significant_neighbor = next((n for n in sorted_neighbors if not n.startswith('u_') and not n == movie), None)\n        if most_significant_neighbor:\n            significant_neighbors[movie] = most_significant_neighbor\n        contributing_paths_all[movie] = contributing_paths\n\n\n        \n    \n    return sorted_movies, contributing_paths_all, significant_neighbors\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:10:28.383963Z","iopub.execute_input":"2023-10-15T17:10:28.384637Z","iopub.status.idle":"2023-10-15T17:10:28.405712Z","shell.execute_reply.started":"2023-10-15T17:10:28.384585Z","shell.execute_reply":"2023-10-15T17:10:28.404074Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"user = train_data['userId'].sample().iloc[0]\nprint(user)\nmovies, paths, neigbors = personalized_pagerank_recommendations(G,user)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:10:28.406969Z","iopub.execute_input":"2023-10-15T17:10:28.408225Z","iopub.status.idle":"2023-10-15T17:10:28.552497Z","shell.execute_reply.started":"2023-10-15T17:10:28.408162Z","shell.execute_reply":"2023-10-15T17:10:28.550556Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"25\n","output_type":"stream"}]},{"cell_type":"code","source":"def id_to_name(movie_id, movies_df):\n    movie_row = movies_df[movies_df['movieId'] == movie_id]\n    if not movie_row.empty:\n        return movie_row['title'].iloc[0]\n    else:\n        return None","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:10:28.554561Z","iopub.execute_input":"2023-10-15T17:10:28.554964Z","iopub.status.idle":"2023-10-15T17:10:28.561431Z","shell.execute_reply.started":"2023-10-15T17:10:28.554931Z","shell.execute_reply":"2023-10-15T17:10:28.560243Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"for neig_key, neig_name in neigbors.items():\n    print(f\"{neig_key[2:]} : {neig_name[2:]}\")\n    print(f\"{id_to_name(int(neig_key[2:]), movies_df)} : {id_to_name(int(neig_name[2:]), movies_df)}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:10:28.565370Z","iopub.execute_input":"2023-10-15T17:10:28.566603Z","iopub.status.idle":"2023-10-15T17:10:28.589162Z","shell.execute_reply.started":"2023-10-15T17:10:28.566560Z","shell.execute_reply":"2023-10-15T17:10:28.587390Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"1250 : 1097\nBridge on the River Kwai, The (1957) : E.T. the Extra-Terrestrial (1982)\n1262 : 356\nGreat Escape, The (1963) : Forrest Gump (1994)\n1 : 590\nToy Story (1995) : Dances with Wolves (1990)\n6 : 1198\nHeat (1995) : Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)\n95 : 1193\nBroken Arrow (1996) : One Flew Over the Cuckoo's Nest (1975)\n165 : 1732\nDie Hard: With a Vengeance (1995) : Big Lebowski, The (1998)\n260 : 4993\nStar Wars: Episode IV - A New Hope (1977) : Lord of the Rings: The Fellowship of the Ring, The (2001)\n357 : 2571\nFour Weddings and a Funeral (1994) : Matrix, The (1999)\n368 : 4993\nMaverick (1994) : Lord of the Rings: The Fellowship of the Ring, The (2001)\n708 : 590\nTruth About Cats & Dogs, The (1996) : Dances with Wolves (1990)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### The code below is borrowed from Lab 2. It generates synthetic groups","metadata":{}},{"cell_type":"code","source":"# group_sizes_to_create = [2, 4, 6]  # Group sizes\n# group_similarity_to_create = [\"RANDOM\", \"SIMILAR\", \"DIVERGENT\", \"SIMILAR_ONE_DIVERGENT\"] \n# group_number = 10\n\n# # Print the variables\n# print(group_number, group_sizes_to_create, group_similarity_to_create)#, preprocessed_dataset_folder)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:10:28.591870Z","iopub.execute_input":"2023-10-15T17:10:28.592397Z","iopub.status.idle":"2023-10-15T17:10:28.597702Z","shell.execute_reply.started":"2023-10-15T17:10:28.592360Z","shell.execute_reply":"2023-10-15T17:10:28.596683Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"group_sizes_to_create = [2, 4, 6]  # Group sizes\ngroup_similarity_to_create = [\"RANDOM\"] \ngroup_number = 10\n\n# Print the variables\nprint(group_number, group_sizes_to_create, group_similarity_to_create)#, preprocessed_dataset_folder)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:10:28.599823Z","iopub.execute_input":"2023-10-15T17:10:28.601064Z","iopub.status.idle":"2023-10-15T17:10:28.614604Z","shell.execute_reply.started":"2023-10-15T17:10:28.601002Z","shell.execute_reply":"2023-10-15T17:10:28.612502Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"10 [2, 4, 6] ['RANDOM']\n","output_type":"stream"}]},{"cell_type":"code","source":"# computing similarity (PCC, based on user ratings) between each pair of users\n\nimport numpy as np\n\nuser_matrix = train_data.pivot_table(columns='movieId', index='userId', values='rating')\n\nuser_id_set = set(train_data['userId'])\nuser_id_indexes = user_matrix.index.values\nuser_matrix = user_matrix.fillna(0)\nnumpy_array = user_matrix.to_numpy()\nsim_matrix = np.corrcoef(numpy_array)","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:10:28.616427Z","iopub.execute_input":"2023-10-15T17:10:28.616828Z","iopub.status.idle":"2023-10-15T17:10:28.662761Z","shell.execute_reply.started":"2023-10-15T17:10:28.616795Z","shell.execute_reply":"2023-10-15T17:10:28.661641Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# dataset_folder = \"ml-1m\" #\"ml-1m\"  \n# preprocessed_dataset_folder = \"preprocessed_dataset\" \n\n# # Preprocessing\n# min_ratings_per_user = 100\n# min_ratings_per_item = 100\n\n# # Group generation settings\n# group_sizes_to_create = [2,4,8] # [2, 3, 4, 5, 6, 7, 8]\n# group_similarity_to_create = [\"RANDOM\", \"SIMILAR_ONE_DIVERGENT\"]\n# group_number = 20\n# similar_threshold = 0.5\n# dissimilar_threshold = -0.1\n# shared_ratings = 5\n\n# # Evaluation settings\n# group_types = \"SYNTHETIC\"  # (only \"SYNTHETIC\" available for ml1m)\n# evaluation_ground_truth = \"USER_RATINGS\" # \"GROUP_CHOICES\", \"USER_SATISFACTION\" # (only \"USER_RATINGS\" available for ml1m)\n# group_sizes_to_test = [2,4,8]\n# group_similarity_to_test = [\"RANDOM\", \"SIMILAR_ONE_DIVERGENT\"] #[\"RANDOM\", \"SIMILAR\", \"DIVERGENT\", \"SIMILAR_ONE_DIVERGENT\"]\n# individual_rs_strategy = \"LENSKIT_CF_ITEM\"  # the used strategy for individual RS\n# aggregation_strategies = [\"BASE\", \"GFAR\", \"EPFuzzDA\"]  # [\"BASE\", \"GFAR\", \"EPFuzzDA\"] list of implemented aggregation strategies we want to test\n# recommendations_number = 20  # number of recommended items\n# individual_rs_validation_folds_k = 0 #validation fold for training hyperparameters of individual RS\n\n# evaluation_strategy = \"COUPLED\"  # \"COUPLED\", \"DECOUPLED\" evaluation type (see https://dl.acm.org/doi/10.1145/3511047.3537650)\n\n# inverse_propensity_debiasing = False #For COUPLED only: True / False whether to normalize feedback with self-normalized inverse propensity score (see https://dl.acm.org/doi/10.1145/3511047.3537650)\n# inverse_propensity_gamma = 0.1 #gamma parameter of the inverse propensity weighting. Larger values indicate more penalization for popular items\n\n# binarize_feedback = False #True / False whether to binarize user feedback for the evaluation\n# binarize_feedback_positive_threshold = 4.0 # if the feedback should be binarized, this denotes the minimal positive value\n\n# feedback_polarity_debiasing = -3.0 #polarity debiasing parameter c from https://dl.acm.org/doi/10.1145/3511047.3537650 usage: rating = max(0, rating+c)\n\n# metrics = [\"NDCG\",\"BINARY\"]  # list of implemented metrics to evaluate\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:10:28.664320Z","iopub.execute_input":"2023-10-15T17:10:28.664809Z","iopub.status.idle":"2023-10-15T17:10:28.672332Z","shell.execute_reply.started":"2023-10-15T17:10:28.664767Z","shell.execute_reply":"2023-10-15T17:10:28.670844Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# import sys\n# sys.path.append('/kaggle/input/setting')","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:10:28.674211Z","iopub.execute_input":"2023-10-15T17:10:28.674741Z","iopub.status.idle":"2023-10-15T17:10:28.693173Z","shell.execute_reply.started":"2023-10-15T17:10:28.674684Z","shell.execute_reply":"2023-10-15T17:10:28.691619Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from abc import ABC, abstractmethod\n\nimport random\nimport numpy as np\n# import config as cfg\n\nrandom.seed(1010101)\n\n# generation of random, similar and divergent groups revised from Kaya et al\n# https://github.com/mesutkaya/recsys2020/blob/8a8c7088bebc3309b8517f62248386ea7be39776/GFAR_python/create_group.py\n\nclass GroupsGenerator(ABC):\n\n    @staticmethod\n    def getGroupsGenerator(type):\n        if type == \"RANDOM\":\n            return RandomGroupsGenerator()\n        elif type == \"SIMILAR\":\n            return SimilarGroupsGenerator()\n        elif type == \"DIVERGENT\":\n            return DivergentGroupsGenerator()\n        elif type == \"SIMILAR_ONE_DIVERGENT\":\n            return MinorityGroupsGenerator()\n        return None\n\n    @staticmethod\n    def compute_average_similarity(group, user_id_indexes, sim_matrix):\n        similarities = list()\n        for user_1 in group:\n            user_1_index = user_id_indexes.tolist().index(user_1)\n            for user_2 in group:\n                user_2_index = user_id_indexes.tolist().index(user_2)\n                if user_1 != user_2:\n                    similarities.append(sim_matrix[user_1_index][user_2_index])\n        return np.mean(similarities)\n\n    @abstractmethod\n    def generateGroups(self, user_id_indexes, user_id_set, similarity_matrix, group_sizes_to_create, group_number_to_create):\n        pass\n\n\nclass RandomGroupsGenerator(GroupsGenerator):\n\n    def generateGroups(self, user_id_indexes, user_id_set, similarity_matrix, group_sizes_to_create, group_number_to_create):\n        groups_list = list()\n        for group_size in group_sizes_to_create:\n            for i in range(group_number_to_create):\n                group = random.sample(user_id_set, group_size)\n                groups_list.append(\n                    {\n                        \"group_size\": group_size,\n                        \"group_similarity\": 'random',\n                        \"group_members\": group,\n                        \"avg_similarity\": GroupsGenerator.compute_average_similarity(group, user_id_indexes, similarity_matrix)\n                    }\n                )\n            print(len(groups_list))\n        return groups_list\n\n\n# class SimilarGroupsGenerator(GroupsGenerator):\n\n#     @staticmethod\n#     def select_user_for_sim_group(group, sim_matrix, user_id_indexes, sim_threshold=0.4):\n#         '''\n#         Helper function to the generate_similar_user_groups function. Given already selected group members, it randomly\n#         selects from the remaining users that has a PCC value >= sim_threshold to any of the existing members.\n#         :param group:\n#         :param sim_matrix:\n#         :param user_id_indexes:\n#         :param sim_threshold:\n#         :return:\n#         '''\n#         ids_to_select_from = set()\n#         for member in group:\n#             member_index = user_id_indexes.tolist().index(member)\n#             indexes = np.where(sim_matrix[member_index] >= sim_threshold)[0].tolist()\n#             user_ids = [user_id_indexes[index] for index in indexes]\n#             ids_to_select_from = ids_to_select_from.union(set(user_ids))\n#         candidate_ids = ids_to_select_from.difference(set(group))\n#         if len(candidate_ids) == 0:\n#             return None\n#         else:\n#             selection = random.sample(candidate_ids, 1)\n#             return selection[0]\n\n#     def generateGroups(self, user_id_indexes, user_id_set, similarity_matrix, group_sizes_to_create, group_number_to_create):\n#         groups_list = list()\n#         for group_size in group_sizes_to_create:\n#             groups_size_list = list()\n#             while (len(groups_size_list) < group_number_to_create):\n#                 group = random.sample(user_id_set, 1)\n#                 while len(group) < group_size:\n#                     new_member = SimilarGroupsGenerator.select_user_for_sim_group(group, similarity_matrix,\n#                                                                                   user_id_indexes,\n#                                                                                   sim_threshold=cfg.similar_threshold)\n#                     if new_member is None:\n#                         break\n#                     group.append(new_member)\n#                 if len(group) == group_size:\n#                     groups_size_list.append(\n#                         {\n#                             \"group_size\": group_size,\n#                             \"group_similarity\": 'similar',\n#                             \"group_members\": group,\n#                             \"avg_similarity\": GroupsGenerator.compute_average_similarity(group, user_id_indexes, similarity_matrix)\n#                         }\n#                     )\n#             groups_list.extend(groups_size_list)\n#             print(len(groups_list))\n#         return groups_list\n\n\n# class DivergentGroupsGenerator(GroupsGenerator):\n\n#     @staticmethod\n#     def select_user_for_divergent_group(group, sim_matrix, user_id_indexes, sim_threshold=0.0):\n#         '''\n#         Helper function to the generate_similar_user_groups function. Given already selected group members, it randomly\n#         selects from the remaining users that has a PCC value < sim_threshold to any of the existing members.\n#         :param group:\n#         :param sim_matrix:\n#         :param user_id_indexes:\n#         :param sim_threshold:\n#         :return:\n#         '''\n#         ids_to_select_from = set()\n#         for member in group:\n#             member_index = user_id_indexes.tolist().index(member)\n#             indexes = np.where(sim_matrix[member_index] < sim_threshold)[0].tolist()\n#             user_ids = [user_id_indexes[index] for index in indexes]\n#             ids_to_select_from = ids_to_select_from.union(set(user_ids))\n#         candidate_ids = ids_to_select_from.difference(set(group))\n#         if len(candidate_ids) == 0:\n#             return None\n#         else:\n#             selection = random.sample(candidate_ids, 1)\n#             return selection[0]\n\n#     def generateGroups(self, user_id_indexes, user_id_set, similarity_matrix, group_sizes_to_create, group_number_to_create):\n#         groups_list = list()\n#         for group_size in group_sizes_to_create:\n#             groups_size_list = list()\n#             while (len(groups_size_list) < group_number_to_create):\n#                 group = random.sample(user_id_set, 1)\n#                 while len(group) < group_size:\n#                     new_member = DivergentGroupsGenerator.select_user_for_divergent_group(group, similarity_matrix,\n#                                                                                      user_id_indexes,\n#                                                                                      sim_threshold=cfg.dissimilar_threshold)\n#                     if new_member is None:\n#                         break\n#                     group.append(new_member)\n#                 if len(group) == group_size:\n#                     groups_size_list.append(\n#                         {\n#                             \"group_size\": group_size,\n#                             \"group_similarity\": 'divergent',\n#                             \"group_members\": group,\n#                             \"avg_similarity\": GroupsGenerator.compute_average_similarity(group, user_id_indexes, similarity_matrix)\n#                         }\n#                     )\n#             groups_list.extend(groups_size_list)\n#             print(len(groups_list))\n#         return groups_list\n\n\n# class MinorityGroupsGenerator(GroupsGenerator):\n#     def generateGroups(self, user_id_indexes, user_id_set, similarity_matrix, group_sizes_to_create, group_number_to_create):\n#         groups_list = list()\n#         for group_size in group_sizes_to_create:\n#             groups_size_list = list()\n#             while (len(groups_size_list) < group_number_to_create):\n#                 group = random.sample(user_id_set, 1)\n#                 while len(group) < (group_size - 1):\n#                     new_member = SimilarGroupsGenerator.select_user_for_sim_group(group, similarity_matrix,\n#                                                                                      user_id_indexes,\n#                                                                                      sim_threshold=cfg.similar_threshold)\n#                     if new_member is None:\n#                         break\n#                     group.append(new_member)\n\n#                 dissimilar_member = DivergentGroupsGenerator.select_user_for_divergent_group(group, similarity_matrix,\n#                                                                                               user_id_indexes,\n#                                                                                               sim_threshold=cfg.dissimilar_threshold)\n#                 if dissimilar_member is not None:\n#                     group.append(dissimilar_member)\n#                 if len(group) == group_size:\n#                     groups_size_list.append(\n#                         {\n#                             \"group_size\": group_size,\n#                             \"group_similarity\": 'similar_one_divergent',\n#                             \"group_members\": group,\n#                             \"avg_similarity\": GroupsGenerator.compute_average_similarity(group, user_id_indexes, similarity_matrix)\n#                         }\n#                     )\n#             groups_list.extend(groups_size_list)\n#             print(len(groups_list))\n#         return groups_list\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:10:28.695045Z","iopub.execute_input":"2023-10-15T17:10:28.695481Z","iopub.status.idle":"2023-10-15T17:10:28.714762Z","shell.execute_reply.started":"2023-10-15T17:10:28.695450Z","shell.execute_reply":"2023-10-15T17:10:28.713081Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"group_list = list()\nfor group_type in group_similarity_to_create:\n    print(group_type)\n    grpGenerator = GroupsGenerator.getGroupsGenerator(group_type)\n    current_list = grpGenerator.generateGroups(user_id_indexes, user_id_set, sim_matrix, group_sizes_to_create, group_number)\n    \n    display(pd.DataFrame.from_records(current_list))\n    \n    group_list = group_list + current_list","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:10:28.716362Z","iopub.execute_input":"2023-10-15T17:10:28.716720Z","iopub.status.idle":"2023-10-15T17:10:28.758568Z","shell.execute_reply.started":"2023-10-15T17:10:28.716691Z","shell.execute_reply":"2023-10-15T17:10:28.757833Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"RANDOM\n10\n20\n30\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_32/3765531497.py:48: DeprecationWarning: Sampling from a set deprecated\nsince Python 3.9 and will be removed in a subsequent version.\n  group = random.sample(user_id_set, group_size)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"    group_size group_similarity             group_members  avg_similarity\n0            2           random                  [76, 85]       -0.042904\n1            2           random                   [83, 3]        0.058024\n2            2           random                  [62, 71]        0.163796\n3            2           random                  [33, 87]       -0.028435\n4            2           random                   [28, 8]        0.435992\n5            2           random                  [51, 64]        0.011140\n6            2           random                  [82, 76]       -0.029327\n7            2           random                   [6, 90]        0.005481\n8            2           random                   [3, 71]       -0.040065\n9            2           random                  [54, 34]        0.058122\n10           4           random           [12, 38, 6, 63]        0.103302\n11           4           random           [59, 58, 4, 21]        0.043239\n12           4           random           [62, 12, 75, 8]        0.186185\n13           4           random            [52, 4, 7, 20]       -0.010607\n14           4           random          [74, 40, 55, 50]        0.039323\n15           4           random          [73, 60, 68, 37]        0.026423\n16           4           random          [37, 11, 24, 84]        0.036132\n17           4           random           [65, 49, 10, 7]        0.006503\n18           4           random          [24, 52, 59, 61]        0.018082\n19           4           random          [90, 49, 30, 77]        0.002722\n20           6           random  [34, 76, 27, 39, 29, 16]        0.018214\n21           6           random  [43, 90, 53, 84, 70, 23]        0.050478\n22           6           random  [69, 80, 35, 64, 73, 45]        0.013930\n23           6           random  [25, 40, 84, 71, 14, 59]        0.027302\n24           6           random   [12, 59, 31, 60, 2, 30]        0.043129\n25           6           random  [68, 71, 81, 86, 37, 39]       -0.026960\n26           6           random  [39, 75, 46, 18, 37, 63]        0.005304\n27           6           random   [52, 56, 24, 34, 2, 18]        0.050815\n28           6           random  [90, 88, 10, 91, 81, 13]        0.052929\n29           6           random   [24, 7, 50, 13, 41, 38]        0.056012","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>group_size</th>\n      <th>group_similarity</th>\n      <th>group_members</th>\n      <th>avg_similarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>random</td>\n      <td>[76, 85]</td>\n      <td>-0.042904</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>random</td>\n      <td>[83, 3]</td>\n      <td>0.058024</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>random</td>\n      <td>[62, 71]</td>\n      <td>0.163796</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>random</td>\n      <td>[33, 87]</td>\n      <td>-0.028435</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>random</td>\n      <td>[28, 8]</td>\n      <td>0.435992</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2</td>\n      <td>random</td>\n      <td>[51, 64]</td>\n      <td>0.011140</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2</td>\n      <td>random</td>\n      <td>[82, 76]</td>\n      <td>-0.029327</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2</td>\n      <td>random</td>\n      <td>[6, 90]</td>\n      <td>0.005481</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2</td>\n      <td>random</td>\n      <td>[3, 71]</td>\n      <td>-0.040065</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2</td>\n      <td>random</td>\n      <td>[54, 34]</td>\n      <td>0.058122</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>4</td>\n      <td>random</td>\n      <td>[12, 38, 6, 63]</td>\n      <td>0.103302</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>4</td>\n      <td>random</td>\n      <td>[59, 58, 4, 21]</td>\n      <td>0.043239</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>4</td>\n      <td>random</td>\n      <td>[62, 12, 75, 8]</td>\n      <td>0.186185</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>4</td>\n      <td>random</td>\n      <td>[52, 4, 7, 20]</td>\n      <td>-0.010607</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>4</td>\n      <td>random</td>\n      <td>[74, 40, 55, 50]</td>\n      <td>0.039323</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>4</td>\n      <td>random</td>\n      <td>[73, 60, 68, 37]</td>\n      <td>0.026423</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>4</td>\n      <td>random</td>\n      <td>[37, 11, 24, 84]</td>\n      <td>0.036132</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>4</td>\n      <td>random</td>\n      <td>[65, 49, 10, 7]</td>\n      <td>0.006503</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>4</td>\n      <td>random</td>\n      <td>[24, 52, 59, 61]</td>\n      <td>0.018082</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>4</td>\n      <td>random</td>\n      <td>[90, 49, 30, 77]</td>\n      <td>0.002722</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>6</td>\n      <td>random</td>\n      <td>[34, 76, 27, 39, 29, 16]</td>\n      <td>0.018214</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>6</td>\n      <td>random</td>\n      <td>[43, 90, 53, 84, 70, 23]</td>\n      <td>0.050478</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>6</td>\n      <td>random</td>\n      <td>[69, 80, 35, 64, 73, 45]</td>\n      <td>0.013930</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>6</td>\n      <td>random</td>\n      <td>[25, 40, 84, 71, 14, 59]</td>\n      <td>0.027302</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>6</td>\n      <td>random</td>\n      <td>[12, 59, 31, 60, 2, 30]</td>\n      <td>0.043129</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>6</td>\n      <td>random</td>\n      <td>[68, 71, 81, 86, 37, 39]</td>\n      <td>-0.026960</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>6</td>\n      <td>random</td>\n      <td>[39, 75, 46, 18, 37, 63]</td>\n      <td>0.005304</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>6</td>\n      <td>random</td>\n      <td>[52, 56, 24, 34, 2, 18]</td>\n      <td>0.050815</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>6</td>\n      <td>random</td>\n      <td>[90, 88, 10, 91, 81, 13]</td>\n      <td>0.052929</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>6</td>\n      <td>random</td>\n      <td>[24, 7, 50, 13, 41, 38]</td>\n      <td>0.056012</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# group_list[30]#['group_members']","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:10:28.759358Z","iopub.execute_input":"2023-10-15T17:10:28.759614Z","iopub.status.idle":"2023-10-15T17:10:28.764087Z","shell.execute_reply.started":"2023-10-15T17:10:28.759591Z","shell.execute_reply":"2023-10-15T17:10:28.763219Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Input: all movies ranked x number of users in a group \n# Those movies would be produced by one of heruistic algorithms in Rafael\n\n# users = group_list[22]['group_members']\n\n\ndef combine_preference_of_indiv_to_make_group_recommendation(G,users):\n    # ranked_movies_list, _, _ = personalized_pagerank_recommendations(G, users[0])\n    # print(ranked_movies_list)\n    \n    common_movies = []\n    key_list = users\n    all_users_movie_ranking = {key: None for key in key_list}\n    #print(all_users_movie_ranking)\n\n    # run RafGraph to find the ranking of all the movies per users\n    \n    for i in range (0,len(users)):\n        print(i)\n        ranked_movies_list, _, _ = personalized_pagerank_recommendations(G, users[i])\n        all_users_movie_ranking[users[i]] = ranked_movies_list\n        \n        # Make all the movie lists such that they contain the same set of movies\n        if not common_movies:\n            common_movies = ranked_movies_list\n        else:\n            common_movies = list(set(common_movies) & set(ranked_movies_list))\n        \n        \n    for key in all_users_movie_ranking:\n        all_users_movie_ranking[key] = [movie for movie in all_users_movie_ranking[key] if movie in common_movies]\n#    # print(all_users_movie_ranking)\n    \n    global_ranking_df = pd.DataFrame(index=common_movies, columns=key_list)\n    global_ranking_df = global_ranking_df.fillna(0)\n    \n    # Filling in the values\n    for i in range (0,len(users)):\n        ranked_movies = all_users_movie_ranking[users[i]]\n        k = 1\n        for movie in ranked_movies:\n            global_ranking_df.at[movie, users[i]] += k\n            k+=1\n            \n    global_ranking_df['Ranking Position'] = global_ranking_df.mean(axis=1)\n    global_ranking_df = global_ranking_df.sort_values(by='Ranking Position')\n    \n    #suggested_movie = global_ranking_df['Ranking Position'].index[0]\n    avg_ranking = global_ranking_df['Ranking Position'][0]\n    print(global_ranking_df['Ranking Position'][0])\n    print(global_ranking_df)\n    \n    return avg_ranking\n    \n# print(combine_preference_of_indiv_to_make_group_recommendation(G, users))\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:10:28.765447Z","iopub.execute_input":"2023-10-15T17:10:28.765784Z","iopub.status.idle":"2023-10-15T17:10:28.783717Z","shell.execute_reply.started":"2023-10-15T17:10:28.765743Z","shell.execute_reply":"2023-10-15T17:10:28.782251Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"#### Random Group Composition","metadata":{}},{"cell_type":"code","source":"average_ranking_random = []\ngroup_sizes = [2, 4, 6]\n\nfor i in range (0, 30):\n    users = group_list[i]['group_members']\n    print(users)\n    \n    if group_list[i]['group_similarity'] == 'random':\n        average_ranking_random.append(combine_preference_of_indiv_to_make_group_recommendation(G, users))\n\nprint(average_ranking_random)\n\n# Split the list into 3 groups of 10 values each\ngroups = [average_ranking_random[i:i+10] for i in range(0, len(average_ranking_random), 10)]\n\n# Calculate the average of values in each group\naverages_random = [np.mean(group) for group in groups]\n\nprint(\"For random group composition: \")\n\n# Print the averages for each group\nfor i, avg in enumerate(averages_random):\n    print('Group size: ', group_sizes[i])\n    print(f'Group {i+1} Average: {avg:.2f}')   ","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:10:28.785326Z","iopub.execute_input":"2023-10-15T17:10:28.785702Z","iopub.status.idle":"2023-10-15T17:11:14.021509Z","shell.execute_reply.started":"2023-10-15T17:10:28.785671Z","shell.execute_reply":"2023-10-15T17:11:14.019133Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"[76, 85]\n0\n1\n5.5\n             76     85  Ranking Position\nm_2762        6      5               5.5\nm_318        17      1               9.0\nm_337        18      3              10.5\nm_356        19     11              15.0\nm_357        20     12              16.0\n...         ...    ...               ...\nm_131254  27235  27235           27235.0\nm_131256  27236  27236           27236.0\nm_131258  27237  27237           27237.0\nm_131260  27238  27238           27238.0\nm_131262  27239  27239           27239.0\n\n[27239 rows x 3 columns]\n[83, 3]\n0\n1\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(users)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m group_list[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup_similarity\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 9\u001b[0m         average_ranking_random\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcombine_preference_of_indiv_to_make_group_recommendation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musers\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(average_ranking_random)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Split the list into 3 groups of 10 values each\u001b[39;00m\n","Cell \u001b[0;32mIn[19], line 31\u001b[0m, in \u001b[0;36mcombine_preference_of_indiv_to_make_group_recommendation\u001b[0;34m(G, users)\u001b[0m\n\u001b[1;32m     27\u001b[0m             common_movies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(common_movies) \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mset\u001b[39m(ranked_movies_list))\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m all_users_movie_ranking:\n\u001b[0;32m---> 31\u001b[0m         all_users_movie_ranking[key] \u001b[38;5;241m=\u001b[39m [movie \u001b[38;5;28;01mfor\u001b[39;00m movie \u001b[38;5;129;01min\u001b[39;00m all_users_movie_ranking[key] \u001b[38;5;28;01mif\u001b[39;00m movie \u001b[38;5;129;01min\u001b[39;00m common_movies]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#    # print(all_users_movie_ranking)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     global_ranking_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(index\u001b[38;5;241m=\u001b[39mcommon_movies, columns\u001b[38;5;241m=\u001b[39mkey_list)\n","Cell \u001b[0;32mIn[19], line 31\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     27\u001b[0m             common_movies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(common_movies) \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mset\u001b[39m(ranked_movies_list))\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m all_users_movie_ranking:\n\u001b[0;32m---> 31\u001b[0m         all_users_movie_ranking[key] \u001b[38;5;241m=\u001b[39m [movie \u001b[38;5;28;01mfor\u001b[39;00m movie \u001b[38;5;129;01min\u001b[39;00m all_users_movie_ranking[key] \u001b[38;5;28;01mif\u001b[39;00m movie \u001b[38;5;129;01min\u001b[39;00m common_movies]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#    # print(all_users_movie_ranking)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     global_ranking_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(index\u001b[38;5;241m=\u001b[39mcommon_movies, columns\u001b[38;5;241m=\u001b[39mkey_list)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Sample data\ngroup_sizes = [2, 4, 6]\naverage_satisfaction_random = [5.9, 12.10, 12.78]\n\n# Create a bar chart\nplt.figure(figsize=(10, 6))\nbars = plt.bar(range(len(group_sizes)), average_satisfaction_random, tick_label=group_sizes)\nplt.xlabel(\"Group Size\")\nplt.ylabel(\"Average Ranking Position\")\nplt.title(\"Average Satisfaction by Group Size. Random Group Composition\")\nplt.xticks(range(len(group_sizes)), group_sizes)\n\n# Add values above the bars\nfor bar, value in zip(bars, average_satisfaction_random):\n    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1, f\"{value:.2f}\", ha='center', va='bottom')\n\nplt.savefig('random.png')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:11:14.022600Z","iopub.status.idle":"2023-10-15T17:11:14.022983Z","shell.execute_reply.started":"2023-10-15T17:11:14.022804Z","shell.execute_reply":"2023-10-15T17:11:14.022822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Similar Group Composition","metadata":{}},{"cell_type":"code","source":"# average_ranking_similar = []\n# group_sizes = [2, 4, 6]\n\n# for i in range (30, 60):\n#     users = group_list[i]['group_members']\n#     print(users)\n    \n#     if group_list[i]['group_similarity'] == 'similar':\n#         average_ranking_similar.append(combine_preference_of_indiv_to_make_group_recommendation(G, users))    \n\n# print(len(average_ranking_similar))\n\n# # Split the list into 4 groups of 10 values each\n# groups = [average_ranking_similar[i:i+10] for i in range(0, len(average_ranking_similar), 10)]\n\n# # Calculate the average of values in each group\n# averages_similar = [np.mean(group) for group in groups]\n\n# print(\"For random group composition: \")\n\n# # Print the averages for each group\n# for i, avg in enumerate(averages_similar):\n#     print('Group size: ', group_sizes[i])\n#     print(f'Average: {avg:.2f}')   ","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:11:14.024953Z","iopub.status.idle":"2023-10-15T17:11:14.025442Z","shell.execute_reply.started":"2023-10-15T17:11:14.025245Z","shell.execute_reply":"2023-10-15T17:11:14.025265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# # Sample data\n# group_sizes = [2, 4, 6]\n# average_satisfaction_similar = [4.70, 17.38, 22.60]\n\n# # Create a bar chart\n# plt.figure(figsize=(10, 6))\n# bars = plt.bar(range(len(group_sizes)), average_satisfaction_similar, tick_label=group_sizes)\n# plt.xlabel(\"Group Size\")\n# plt.ylabel(\"Average Ranking Position\")\n# plt.title(\"Average Satisfaction by Group Size. Similar Group Composition\")\n# plt.xticks(range(len(group_sizes)), group_sizes)\n\n# # Add values above the bars\n# for bar, value in zip(bars, average_satisfaction_similar):\n#     plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1, f\"{value:.2f}\", ha='center', va='bottom')\n\n# plt.savefig('similar.png')\n# plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:11:14.026703Z","iopub.status.idle":"2023-10-15T17:11:14.027317Z","shell.execute_reply.started":"2023-10-15T17:11:14.026919Z","shell.execute_reply":"2023-10-15T17:11:14.026937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Divergent Group Composition","metadata":{}},{"cell_type":"code","source":"# average_ranking_divergent = []\n# group_sizes = [2, 4, 6]\n\n# for i in range (60, 90):\n#     users = group_list[i]['group_members']\n#     print(users)\n    \n#     if group_list[i]['group_similarity'] == 'divergent':\n#         average_ranking_divergent.append(combine_preference_of_indiv_to_make_group_recommendation(G, users))    \n\n# print(len(average_ranking_divergent))\n\n# # Split the list into 4 groups of 10 values each\n# groups = [average_ranking_divergent[i:i+10] for i in range(0, len(average_ranking_divergent), 10)]\n\n# # Calculate the average of values in each group\n# averages_divergent = [np.mean(group) for group in groups]\n\n# print(\"For divergent group composition: \")\n\n# # Print the averages for each group\n# for i, avg in enumerate(averages_divergent):\n#     print('Group size: ', group_sizes[i])\n#     print(f'Average: {avg:.2f}')   ","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:11:14.028934Z","iopub.status.idle":"2023-10-15T17:11:14.029361Z","shell.execute_reply.started":"2023-10-15T17:11:14.029141Z","shell.execute_reply":"2023-10-15T17:11:14.029192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# # Sample data\n# group_sizes = [2, 4, 6]\n# average_satisfaction_divergent = [7.30, 13.40, 10.37]\n\n# # Create a bar chart\n# plt.figure(figsize=(10, 6))\n# bars = plt.bar(range(len(group_sizes)), average_satisfaction_divergent, tick_label=group_sizes)\n# plt.xlabel(\"Group Size\")\n# plt.ylabel(\"Average Ranking Position\")\n# plt.title(\"Average Satisfaction by Group Size. Givergent Group Composition\")\n# plt.xticks(range(len(group_sizes)), group_sizes)\n\n# # Add values above the bars\n# for bar, value in zip(bars, average_satisfaction_divergent):\n#     plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1, f\"{value:.2f}\", ha='center', va='bottom')\n\n# plt.savefig('divergent.png')\n# plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:11:14.032613Z","iopub.status.idle":"2023-10-15T17:11:14.033075Z","shell.execute_reply.started":"2023-10-15T17:11:14.032885Z","shell.execute_reply":"2023-10-15T17:11:14.032906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# average_ranking_minority = []\n# group_sizes = [2, 4, 6]\n\n# for i in range (90, 120):\n#     users = group_list[i]['group_members']\n#     print(users)\n    \n#     if group_list[i]['group_similarity'] == 'similar_one_divergent':\n#         average_ranking_minority.append(combine_preference_of_indiv_to_make_group_recommendation(G, users))    \n\n# print(len(average_ranking_minority))\n\n# # Split the list into 4 groups of 10 values each\n# groups = [average_ranking_minority[i:i+10] for i in range(0, len(average_ranking_minority), 10)]\n\n# # Calculate the average of values in each group\n# averages_minority = [np.mean(group) for group in groups]\n\n# print(\"For minority group composition: \")\n\n# # Print the averages for each group\n# for i, avg in enumerate(averages_minority):\n#     print('Group size: ', group_sizes[i])\n#     print(f'Average: {avg:.2f}')   ","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:11:14.034729Z","iopub.status.idle":"2023-10-15T17:11:14.035187Z","shell.execute_reply.started":"2023-10-15T17:11:14.034974Z","shell.execute_reply":"2023-10-15T17:11:14.034994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# # Sample data\n# group_sizes = [2, 4, 6]\n# average_ranking_minority = [5.6, 15.40, 19.18]\n\n# # Create a bar chart\n# plt.figure(figsize=(10, 6))\n# bars = plt.bar(range(len(group_sizes)), average_ranking_minority, tick_label=group_sizes)\n# plt.xlabel(\"Group Size\")\n# plt.ylabel(\"Average Ranking Position\")\n# plt.title(\"Average Satisfaction by Group Size. Minority Group Composition\")\n# plt.xticks(range(len(group_sizes)), group_sizes)\n\n# # Add values above the bars\n# for bar, value in zip(bars, average_ranking_minority):\n#     plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1, f\"{value:.2f}\", ha='center', va='bottom')\n\n# plt.savefig('minority.png')\n# plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-15T17:11:14.037475Z","iopub.status.idle":"2023-10-15T17:11:14.038256Z","shell.execute_reply.started":"2023-10-15T17:11:14.038021Z","shell.execute_reply":"2023-10-15T17:11:14.038042Z"},"trusted":true},"execution_count":null,"outputs":[]}]}