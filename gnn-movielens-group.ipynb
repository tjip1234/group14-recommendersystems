{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom networkx.algorithms import bipartite\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport random","metadata":{"execution":{"iopub.status.busy":"2023-10-12T07:06:43.812906Z","iopub.execute_input":"2023-10-12T07:06:43.813671Z","iopub.status.idle":"2023-10-12T07:06:48.846695Z","shell.execute_reply.started":"2023-10-12T07:06:43.813638Z","shell.execute_reply":"2023-10-12T07:06:48.845748Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the data\ndata = pd.read_csv('/kaggle/input/movielens-20m-dataset/rating.csv')\nmovies_df = pd.read_csv(\"/kaggle/input/movielens-20m-dataset/movie.csv\")\ntags_df = pd.read_csv(\"/kaggle/input/movielens-20m-dataset/tag.csv\")\n\n# Convert the timestamp column to datetime format for sorting\ndata['timestamp'] = pd.to_datetime(data['timestamp'])\n\n# Filter users who have watched at least 30 movies\nuser_counts = data['userId'].value_counts()\neligible_users = user_counts[user_counts >= 30].index.tolist()\n\n# Initialize train and test dataframes\ntrain_data = pd.DataFrame()\ntest_data = pd.DataFrame()\n\n# Iterate over eligible users and split their data\nfor user_id in eligible_users:\n    user_data = data[data['userId'] == user_id].sort_values(by='timestamp', ascending=False)\n    \n    # Add the latest 3 movies to the test set\n    test_data = pd.concat([test_data, user_data.iloc[:3]])\n    \n    # Add the remaining movies to the train set\n    train_data = pd.concat([train_data, user_data.iloc[3:]])\n    \n    # Check if train_data has reached the desired size\n    if 10000 <= len(train_data) <= 50000:\n        break\n\n# Filter the train_data to keep only popular movies\nmovie_num_user_rated_counts = train_data['movieId'].value_counts()\npopular_movies = movie_num_user_rated_counts[movie_num_user_rated_counts >= 10].index.tolist()\ntrain_data = train_data[train_data['movieId'].isin(popular_movies)]\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-12T07:06:49.271112Z","iopub.execute_input":"2023-10-12T07:06:49.271766Z","iopub.status.idle":"2023-10-12T07:07:59.763329Z","shell.execute_reply.started":"2023-10-12T07:06:49.271733Z","shell.execute_reply":"2023-10-12T07:07:59.762215Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"ratings_df = train_data\n# Create a new graph\nG = nx.Graph()\n\n# 2. Adding nodes and edges\n# Add user nodes\nfor user_id in ratings_df['userId'].unique():\n    G.add_node('u_'+str(user_id), bipartite=0)  # Add user node with a bipartite attribute of 0\n\n# Add movie nodes with title and genres as attributes\nfor _, row in movies_df.iterrows():\n    movie_id = 'm_' + str(row['movieId'])\n    G.add_node(movie_id, bipartite=1, title=row['title'], genres=row['genres'].split('|'))\n\n# Add edges based on ratings with rating and timestamp as attributes\nfor _, row in ratings_df.iterrows():\n    user_id = 'u_'+str(row['userId'])\n    movie_id = 'm_' + str(row['movieId'])\n    G.add_edge(user_id, movie_id, rating=row['rating'], timestamp=row['timestamp'])\n\n# Add tag data as an attribute to the movie nodes\nfor _, row in tags_df.iterrows():\n    movie_id = 'm_' + str(row['movieId'])\n    if 'tags' not in G.nodes[movie_id]:\n        G.nodes[movie_id]['tags'] = []\n    G.nodes[movie_id]['tags'].append({'tag': row['tag'], 'timestamp': row['timestamp'], 'userId': row['userId']})\n\n# Ensure the graph is bipartite\nassert bipartite.is_bipartite(G)","metadata":{"execution":{"iopub.status.busy":"2023-10-11T19:02:10.410307Z","iopub.execute_input":"2023-10-11T19:02:10.410620Z","iopub.status.idle":"2023-10-11T19:02:45.020095Z","shell.execute_reply.started":"2023-10-11T19:02:10.410594Z","shell.execute_reply":"2023-10-11T19:02:45.018600Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def predict_rating(G, user, movie):\n    neighbors = list(G.neighbors(movie))\n    if not neighbors:\n        return np.mean([attr['rating'] for _, _, attr in G.edges(data=True) if 'rating' in attr])\n\n    sim_weights = []\n    user_ratings = []\n    for neighbor in neighbors:\n        # Jaccard similarity as an example, but can be changed\n        common_movies = list(nx.common_neighbors(G, user, neighbor))\n        sim = len(common_movies) / (G.degree(user) + G.degree(neighbor) - len(common_movies))\n        rating = G[neighbor][movie]['rating']\n        \n        sim_weights.append(sim)\n        user_ratings.append(rating)\n    \n    return np.dot(user_ratings, sim_weights) / sum(sim_weights)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T19:02:45.023561Z","iopub.execute_input":"2023-10-11T19:02:45.024012Z","iopub.status.idle":"2023-10-11T19:02:45.032866Z","shell.execute_reply.started":"2023-10-11T19:02:45.023979Z","shell.execute_reply":"2023-10-11T19:02:45.031910Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def random_walk_legacy(G, start_node, alpha=0.85, walk_length=10):\n    \"\"\"Perform a random walk on graph G starting from node start_node.\"\"\"\n    current_node = start_node\n    path = [current_node]\n    \n    for _ in range(walk_length):\n        neighbors = list(G.neighbors(current_node))\n        if random.random() < alpha and neighbors:\n            current_node = random.choice(neighbors)\n        else:\n            current_node = start_node\n        path.append(current_node)\n    \n    return path\ndef weighted_choice(neighbors, weights):\n    \"\"\"Choose a neighbor based on the given weights.\"\"\"\n    total = sum(weights)\n    r = random.uniform(0, total)\n    upto = 0\n    for n, w in zip(neighbors, weights):\n        if upto + w >= r:\n            return n\n        upto += w\n\ndef random_walk(G, start_node, alpha=0.85, walk_length=10):\n    \"\"\"Perform a random walk on graph G starting from node start_node.\"\"\"\n    current_node = start_node\n    path = [current_node]\n    \n    for _ in range(walk_length):\n        neighbors = list(G.neighbors(current_node))\n        \n        if neighbors:\n            # Get weights (ratings) of the edges to the neighbors\n            weights = [G[current_node][neighbor].get('rating', 1) for neighbor in neighbors]\n            \n            if random.random() < alpha:\n                current_node = weighted_choice(neighbors, weights)\n            else:\n                current_node = start_node\n        else:\n            current_node = start_node\n            \n        path.append(current_node)\n    \n    return path\ndef personalized_pagerank_recommendations(G, user, alpha=0.85, num_walks=50, walk_length=5):\n    \"\"\"Generate recommendations and explanations using Personalized PageRank via random walks.\"\"\"\n    # Perform random walks and keep track of visit counts\n    visit_counts = {node: 0 for node in G.nodes()}\n    all_paths = []\n    user = 'u_'+str(user)\n    \n    for _ in range(num_walks):\n        path = random_walk(G, user, alpha, walk_length)\n        all_paths.append(path)\n        for node in path:\n            visit_counts[node] += 1\n    \n    # Normalize visit counts to get a probability distribution\n    total_visits = sum(visit_counts.values())\n    ppr = {node: count/total_visits for node, count in visit_counts.items()}\n    \n    # Filter for movies and sort by PPR score\n    movies = [node for node in ppr.keys() if G.nodes[node]['bipartite'] == 1 and node not in G.neighbors(str(user))]\n    sorted_movies = sorted(movies, key=lambda x: ppr[x], reverse=True)\n    \n    # Generate explanations for the top 10 movies\n    explanations = {}\n    significant_neighbors = {}\n    contributing_paths_all = {}\n    for movie in sorted_movies[:10]:\n        # Find paths that contributed to the movie's score\n        contributing_paths = [path for path in all_paths if movie in path]\n        \n        # Count the frequency of each neighbor leading to the movie\n        neighbor_counts = {}\n        for path in contributing_paths:\n            for i in range(len(path) - 1):\n                if path[i+1] == movie:\n                    neighbor = path[1]\n                    neighbor_counts[neighbor] = neighbor_counts.get(neighbor, 0) + 1\n\n        # Identify the most significant neighbor\n        sorted_neighbors = sorted(neighbor_counts, key=neighbor_counts.get, reverse=True)\n        most_significant_neighbor = next((n for n in sorted_neighbors if not n.startswith('u_') and not n == movie), None)\n        if most_significant_neighbor:\n            significant_neighbors[movie] = most_significant_neighbor\n        contributing_paths_all[movie] = contributing_paths\n\n\n        \n    \n    return sorted_movies[:10], contributing_paths_all, significant_neighbors\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T19:02:45.035250Z","iopub.execute_input":"2023-10-11T19:02:45.035601Z","iopub.status.idle":"2023-10-11T19:02:45.059004Z","shell.execute_reply.started":"2023-10-11T19:02:45.035573Z","shell.execute_reply":"2023-10-11T19:02:45.057835Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"user = train_data['userId'].sample().iloc[0]\nmovies, paths, neigbors = personalized_pagerank_recommendations(G,user)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-11T19:02:45.060453Z","iopub.execute_input":"2023-10-11T19:02:45.061696Z","iopub.status.idle":"2023-10-11T19:02:45.186407Z","shell.execute_reply.started":"2023-10-11T19:02:45.061644Z","shell.execute_reply":"2023-10-11T19:02:45.185362Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def id_to_name(movie_id, movies_df):\n    movie_row = movies_df[movies_df['movieId'] == movie_id]\n    if not movie_row.empty:\n        return movie_row['title'].iloc[0]\n    else:\n        return None","metadata":{"execution":{"iopub.status.busy":"2023-10-11T19:02:45.187912Z","iopub.execute_input":"2023-10-11T19:02:45.188897Z","iopub.status.idle":"2023-10-11T19:02:45.194179Z","shell.execute_reply.started":"2023-10-11T19:02:45.188869Z","shell.execute_reply":"2023-10-11T19:02:45.193145Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"for neig_key, neig_name in neigbors.items():\n    print(f\"{neig_key[2:]} : {neig_name[2:]}\")\n    print(f\"{id_to_name(int(neig_key[2:]), movies_df)} : {id_to_name(int(neig_name[2:]), movies_df)}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-11T19:02:45.195973Z","iopub.execute_input":"2023-10-11T19:02:45.197230Z","iopub.status.idle":"2023-10-11T19:02:45.225052Z","shell.execute_reply.started":"2023-10-11T19:02:45.197180Z","shell.execute_reply":"2023-10-11T19:02:45.223276Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"2571 : 595\nMatrix, The (1999) : Beauty and the Beast (1991)\n1676 : 788\nStarship Troopers (1997) : Nutty Professor, The (1996)\n21 : 370\nGet Shorty (1995) : Naked Gun 33 1/3: The Final Insult (1994)\n39 : 595\nClueless (1995) : Beauty and the Beast (1991)\n47 : 1198\nSeven (a.k.a. Se7en) (1995) : Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)\n165 : 316\nDie Hard: With a Vengeance (1995) : Stargate (1994)\n173 : 1136\nJudge Dredd (1995) : Monty Python and the Holy Grail (1975)\n292 : 110\nOutbreak (1995) : Braveheart (1995)\n296 : 593\nPulp Fiction (1994) : Silence of the Lambs, The (1991)\n349 : 11\nClear and Present Danger (1994) : American President, The (1995)\n","output_type":"stream"}]}]}